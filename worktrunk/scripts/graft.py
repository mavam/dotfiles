#!/usr/bin/env -S uv run
# /// script
# dependencies = ["click", "rapidhash"]
# ///
"""
graft: Optimize a new git worktree by copying cached state from the primary worktree.

Usage as worktrunk post-create hook:
  [post-create]
  graft = "~/.config/worktrunk/scripts/graft.py {{ worktree_path }} --remote-url '{{ remote_url }}' --branch '{{ branch }}'"

What it does:
  - Copies file timestamps (preserves build cache validity)
  - Copies submodules with their .git directories (avoids network fetches)
  - Copies CMake build directory and fixes embedded paths
  - Fixes ninja build files (.ninja_log hashes, .ninja_deps paths)
  - Copies Claude Code settings and sets CLAUDE_CODE_TASK_LIST_ID

The CLAUDE_CODE_TASK_LIST_ID is derived from the remote URL (org-repo) and branch name,
enabling isolated task lists per worktree in Claude Code.
"""

from __future__ import annotations

import contextlib
import json
import logging
import os
import re
import shutil
import subprocess
import sys
import threading
import time
from abc import ABC, abstractmethod
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass, field
from pathlib import Path
from typing import TYPE_CHECKING

import click
import rapidhash

if TYPE_CHECKING:
    pass

# =============================================================================
# Configuration
# =============================================================================

# Build directory patterns to detect (relative to worktree root)
BUILD_DIR_PATTERNS = [
    "build/*",  # CMake presets: build/release, build/debug, build/RelWithDebInfo
    "build",  # Single build directory
    ".build",  # Alternative convention
    "_build",  # Another alternative
]

# Lock handling for transient git index contention.
INDEX_LOCK_RETRY_ATTEMPTS = 8
INDEX_LOCK_RETRY_DELAY_SECONDS = 0.25

# Stale lock cleanup is opt-in to avoid deleting a lock owned by a live git process.
STALE_LOCK_MAX_AGE_SECONDS = 15 * 60
STALE_LOCK_CLEANUP_ENV = "GRAFT_REMOVE_STALE_INDEX_LOCK"


# =============================================================================
# URL Parsing and Sanitization
# =============================================================================


def parse_org_repo_from_url(url: str) -> tuple[str, str] | None:
    """Parse org and repo from a GitHub/GitLab remote URL.

    Handles:
      - HTTPS: https://github.com/owner/repo.git
      - SSH: git@github.com:owner/repo.git
      - GitLab nested: gitlab.com/group/subgroup/repo -> (subgroup, repo)

    Returns (org, repo) tuple or None if URL format is unrecognized.
    """
    # Strip .git suffix
    url = url.rstrip("/")
    if url.endswith(".git"):
        url = url[:-4]

    # SSH format: git@host:path
    ssh_match = re.match(r"^git@[^:]+:(.+)$", url)
    if ssh_match:
        path = ssh_match.group(1)
        parts = path.split("/")
        if len(parts) >= 2:
            return (parts[-2], parts[-1])
        return None

    # URL formats with explicit scheme: https://host/path, ssh://host/path, etc.
    scheme_match = re.match(r"^[a-zA-Z][a-zA-Z0-9+.-]*://[^/]+/(.+)$", url)
    if scheme_match:
        path = scheme_match.group(1)
        parts = path.split("/")
        if len(parts) >= 2:
            return (parts[-2], parts[-1])
        return None

    return None


def sanitize_for_id(name: str) -> str:
    """Sanitize a branch name for use in task list ID.

    Replaces /, \\, :, #, ., and spaces with -.
    Collapses multiple - into one and strips leading/trailing -.
    """
    # Replace problematic characters with -
    result = re.sub(r"[/\\:#.\s]+", "-", name)
    # Collapse multiple - into one
    result = re.sub(r"-+", "-", result)
    # Strip leading/trailing -
    return result.strip("-")


# =============================================================================
# UI Module: ANSI codes, logging, spinner
# =============================================================================

ANSI_ESCAPE = re.compile(
    r"\x1b\[[0-9;]*[a-zA-Z]|\x1b\][^\x07]*\x07|\x1b[<>=?]?[0-9;]*[a-zA-Z]"
)

# ANSI codes
DIM = "\033[2m"
RESET = "\033[0m"
BLUE = "\033[34m"
GREEN = "\033[32m"
RED = "\033[31m"
MOVE_UP = "\033[A"
CLEAR_LINE = "\033[2K"
HIDE_CURSOR = "\033[?25l"
SHOW_CURSOR = "\033[?25h"

# Symbols
CHECK = "\u2714"
CROSS = "\u2718"
CIRCLE = "\u25cb"


# =============================================================================
# Logging
# =============================================================================


class _GraftHandler(logging.Handler):
    """Handler that integrates with the spinner."""

    def emit(self, record: logging.LogRecord) -> None:
        message = self.format(record)
        if _active_spinner:
            _active_spinner._clear_for_log()
        if record.levelno >= logging.INFO:
            print(f"{GREEN}{CHECK}{RESET} {message}", flush=True)
        else:
            print(f"{DIM}{CIRCLE} {message}{RESET}", flush=True)


_LOGGER = logging.getLogger("graft")
_LOGGER.addHandler(_GraftHandler())
_LOGGER.propagate = False
_LOGGER.setLevel(logging.INFO)


def _is_verbose() -> bool:
    """Check if verbose logging is enabled."""
    return _LOGGER.isEnabledFor(logging.DEBUG)


def strip_ansi(text: str) -> str:
    """Remove ANSI escape sequences from text."""
    return ANSI_ESCAPE.sub("", text)


_active_spinner: Spinner | None = None


class Spinner:
    """A spinner that can track multiple concurrent tasks."""

    frames = "\u280b\u2819\u2839\u2838\u283c\u2834\u2826\u2827\u2807\u280f"

    def __init__(self) -> None:
        self._frame = 0
        self._stop = threading.Event()
        self._thread: threading.Thread | None = None
        self._lock = threading.Lock()
        # Tasks: {name: {"status": "pending"|"active"|"done", "msg": str}}
        self._tasks: dict[str, dict[str, str]] = {}
        self._task_order: list[str] = []
        self._num_lines = 0
        # Tracks whether the cursor is positioned just below spinner output.
        # Logging can clear spinner lines and leave the cursor at the top.
        self._cursor_below_spinner = False

    def _render(self) -> list[str]:
        """Render all task lines."""
        lines = []
        for name in self._task_order:
            task = self._tasks.get(name)
            if not task:
                continue
            status = task["status"]
            msg = task["msg"]
            if status == "done":
                lines.append(f"{GREEN}{CHECK}{RESET} {DIM}{msg}{RESET}")
            elif status == "active":
                spinner_char = self.frames[self._frame % len(self.frames)]
                lines.append(f"{BLUE}{spinner_char}{RESET} {msg}")
            else:  # pending
                lines.append(f"  {DIM}{msg}{RESET}")
        return lines

    def _clear_for_log(self) -> None:
        """Clear all lines for logging output."""
        with self._lock:
            if self._num_lines > 0 and self._cursor_below_spinner:
                # Only clear when we know the cursor is below the spinner block.
                # This prevents clearing unrelated scrollback lines.
                for _ in range(self._num_lines):
                    print(f"{MOVE_UP}{CLEAR_LINE}", end="")
                print("\r", end="", flush=True)
                self._cursor_below_spinner = False

    def _spin(self) -> None:
        while not self._stop.wait(0.12):
            if _is_verbose():
                # In verbose mode, don't animate - just wait
                continue

            with self._lock:
                lines = self._render()
                prev_num_lines = self._num_lines

                # Move cursor up to overwrite previous output only when we are
                # currently positioned below the spinner block.
                if self._cursor_below_spinner and prev_num_lines > 0:
                    print(f"\033[{prev_num_lines}A", end="")

                # Print each line, clearing to end
                for line in lines:
                    print(f"{CLEAR_LINE}{line}", flush=True)

                # Clear any extra lines from previous render
                for _ in range(prev_num_lines - len(lines)):
                    print(CLEAR_LINE, flush=True)

                # Move cursor back up to end of current content
                extra_lines = prev_num_lines - len(lines)
                if extra_lines > 0:
                    print(f"\033[{extra_lines}A", end="", flush=True)

                self._num_lines = len(lines)
                self._frame += 1
                self._cursor_below_spinner = True

    def start(self, msg: str = "") -> None:
        """Start the spinner with an optional initial single task."""
        global _active_spinner
        if not _is_verbose():
            print(HIDE_CURSOR, end="", flush=True)
        if msg:
            self._tasks["main"] = {"status": "active", "msg": msg}
            self._task_order = ["main"]
        self._stop.clear()
        self._cursor_below_spinner = False
        self._thread = threading.Thread(target=self._spin, daemon=True)
        self._thread.start()
        _active_spinner = self

    def update(self, msg: str) -> None:
        """Update the main task message (single-task mode)."""
        with self._lock:
            if "main" in self._tasks:
                self._tasks["main"]["msg"] = msg

    def add_task(self, name: str, msg: str, status: str = "active") -> None:
        """Add a new task to track."""
        with self._lock:
            self._tasks[name] = {"status": status, "msg": msg}
            if name not in self._task_order:
                self._task_order.append(name)

    def update_task(
        self, name: str, msg: str | None = None, status: str | None = None
    ) -> None:
        """Update a specific task's message or status."""
        with self._lock:
            if name in self._tasks:
                if msg is not None:
                    self._tasks[name]["msg"] = msg
                if status is not None:
                    self._tasks[name]["status"] = status

    def complete_task(self, name: str, msg: str | None = None) -> None:
        """Mark a task as completed."""
        self.update_task(name, msg=msg, status="done")

    def set_tasks(
        self, tasks: list[tuple[str, str]], active: list[str] | None = None
    ) -> None:
        """Set multiple tasks at once. tasks is a list of (name, msg) tuples.
        active is an optional list of task names to mark as initially active."""
        active = active or []
        with self._lock:
            self._tasks = {
                name: {"status": "active" if name in active else "pending", "msg": msg}
                for name, msg in tasks
            }
            self._task_order = [name for name, _ in tasks]

    def start_task(self, name: str) -> None:
        """Mark a task as active/running."""
        self.update_task(name, status="active")

    def stop(self) -> None:
        """Stop the spinner and print final state."""
        global _active_spinner
        if self._stop.is_set():
            return  # Already stopped
        self._stop.set()
        if self._thread:
            self._thread.join()
        _active_spinner = None

        with self._lock:
            num_lines = self._num_lines
            cursor_below_spinner = self._cursor_below_spinner

        # Clear spinner lines
        if num_lines > 0:
            # If logging already cleared the spinner block, the cursor is already
            # at the top of it and we must not move further up.
            if cursor_below_spinner:
                print(f"\033[{num_lines}A", end="")
            for _ in range(num_lines):
                print(f"{CLEAR_LINE}")
            print(f"\033[{num_lines}A", end="")
            with self._lock:
                self._num_lines = 0
                self._cursor_below_spinner = False
        print(SHOW_CURSOR, end="", flush=True)
        # Log completed tasks
        for name in self._task_order:
            task = self._tasks.get(name)
            if task and task["status"] == "done":
                _LOGGER.info(task["msg"])


@contextlib.contextmanager
def spinner(msg: str):
    """Show a spinner while a block executes, then clear the line."""
    s = Spinner()
    s.start(msg)
    try:
        yield s
    finally:
        s.stop()


# =============================================================================
# Git Utilities
# =============================================================================


def find_primary_worktree(worktree_path: Path) -> Path | None:
    """Find the primary worktree (source) for a given worktree."""
    # Get the git common dir - this points to the shared .git directory
    result = run_git(["rev-parse", "--git-common-dir"], cwd=worktree_path)
    if result.returncode != 0:
        return None

    # List all worktrees to find the primary one
    result = run_git(["worktree", "list", "--porcelain"], cwd=worktree_path)
    if result.returncode != 0:
        return None

    # Parse worktree list - first entry is typically the primary
    # Format: worktree /path/to/worktree\nHEAD ...\nbranch ...\n\n
    current_worktree = None
    for line in result.stdout.split("\n"):
        if line.startswith("worktree "):
            path = Path(line[9:])
            if current_worktree is None:
                # First worktree is the primary (main worktree)
                if path.resolve() != worktree_path.resolve():
                    return path
                # If the first worktree IS our target, keep looking
            current_worktree = path
        elif line == "":
            current_worktree = None

    # If we only found one worktree (ours), there's no source to copy from
    return None


def validate_worktrees(source: Path, target: Path) -> str | None:
    """Validate that source and target are valid worktrees of the same repo.

    Returns an error message if validation fails, None if valid.
    """
    # Check both paths exist
    if not source.exists():
        return f"Source path does not exist: {source}"
    if not target.exists():
        return f"Target path does not exist: {target}"

    # Check they're not the same path
    if source.resolve() == target.resolve():
        return "Source and target are the same path"

    # Check both are git worktrees
    for path, name in [(source, "Source"), (target, "Target")]:
        result = run_git(["rev-parse", "--git-dir"], cwd=path)
        if result.returncode != 0:
            return f"{name} is not a git repository: {path}"

    # Check they share the same git common dir (same repo)
    def get_common_dir(path: Path) -> Path | None:
        result = run_git(["rev-parse", "--git-common-dir"], cwd=path)
        if result.returncode != 0:
            return None
        return (path / result.stdout.strip()).resolve()

    source_common = get_common_dir(source)
    target_common = get_common_dir(target)

    if source_common is None:
        return f"Cannot determine git common dir for source: {source}"
    if target_common is None:
        return f"Cannot determine git common dir for target: {target}"
    if source_common != target_common:
        return "Source and target are not worktrees of the same repository"

    return None


@dataclass
class SubmoduleInfo:
    """Information about a submodule."""

    name: str
    path: str
    url: str | None = None


_INDEX_LOCK_ERROR_RE = re.compile(
    r"(index\.lock|Unable to create .*\.lock|Another git process seems to be running)",
    re.IGNORECASE,
)


def _stderr_text(stderr: str | bytes | None) -> str:
    """Normalize stderr from subprocess for error matching/logging."""
    if stderr is None:
        return ""
    if isinstance(stderr, bytes):
        return stderr.decode(errors="replace")
    return stderr


def run_git(
    args: list[str],
    *,
    cwd: Path,
    text: bool = True,
    retry_on_index_lock: bool = False,
) -> subprocess.CompletedProcess:
    """Run a git command with optional retries on transient index.lock contention."""
    cmd = ["git", *args]
    attempts = INDEX_LOCK_RETRY_ATTEMPTS if retry_on_index_lock else 1
    result: subprocess.CompletedProcess | None = None

    for attempt in range(attempts):
        result = subprocess.run(cmd, cwd=cwd, capture_output=True, text=text)
        if result.returncode == 0:
            return result

        stderr = _stderr_text(result.stderr)
        if not retry_on_index_lock or not _INDEX_LOCK_ERROR_RE.search(stderr):
            return result

        # Last attempt, return failure as-is.
        if attempt == attempts - 1:
            return result

        delay = INDEX_LOCK_RETRY_DELAY_SECONDS * (attempt + 1)
        _LOGGER.debug(
            f"Retrying git command after index lock contention ({attempt + 1}/{attempts - 1}): "
            f"`git {' '.join(args)}` in {cwd}"
        )
        time.sleep(delay)

    # Should be unreachable, but keeps type-checkers satisfied.
    if result is None:
        raise RuntimeError(f"git command produced no result: {' '.join(cmd)}")
    return result


def get_submodule_info(worktree_path: Path) -> list[SubmoduleInfo]:
    """Parse .gitmodules and return submodule information."""
    result = run_git(
        ["config", "--file", ".gitmodules", "--get-regexp", r"^submodule\..*\.path$"],
        cwd=worktree_path,
    )
    if result.returncode != 0 or not result.stdout.strip():
        return []

    # Build map of submodule name -> path
    submodule_paths: dict[str, str] = {}
    for line in result.stdout.strip().split("\n"):
        # Format: submodule.<name>.path <path>
        key, path = line.split(maxsplit=1)
        # Strip "submodule." prefix and ".path" suffix to handle names with dots
        name = key[len("submodule.") : -len(".path")]
        submodule_paths[name] = path

    # Get submodule URLs
    url_result = run_git(
        ["config", "--file", ".gitmodules", "--get-regexp", r"^submodule\..*\.url$"],
        cwd=worktree_path,
    )
    submodule_urls: dict[str, str] = {}
    if url_result.returncode == 0:
        for line in url_result.stdout.strip().split("\n"):
            if not line.strip():
                continue
            key, url = line.split(maxsplit=1)
            # Strip "submodule." prefix and ".url" suffix
            name = key[len("submodule.") : -len(".url")]
            submodule_urls[name] = url

    return [
        SubmoduleInfo(name=name, path=path, url=submodule_urls.get(name))
        for name, path in submodule_paths.items()
    ]


def get_git_modules_dir(worktree_path: Path) -> Path | None:
    """Get the shared .git/modules directory for a worktree."""
    result = run_git(["rev-parse", "--git-common-dir"], cwd=worktree_path)
    if result.returncode != 0:
        return None
    return (worktree_path / result.stdout.strip() / "modules").resolve()


def get_worktree_git_dir(worktree_path: Path) -> Path | None:
    """Get the .git directory for a worktree."""
    result = run_git(["rev-parse", "--git-dir"], cwd=worktree_path)
    if result.returncode == 0:
        return (worktree_path / result.stdout.strip()).resolve()
    return None


def remove_stale_lock(worktree_path: Path) -> bool:
    """Optionally remove stale index.lock.

    Auto-removal is disabled by default because a long-running git process can
    legitimately hold the lock. To enable cleanup for very old lock files, set
    GRAFT_REMOVE_STALE_INDEX_LOCK=1.
    """
    git_dir = get_worktree_git_dir(worktree_path)
    if not git_dir:
        return False

    lock_file = git_dir / "index.lock"
    if not lock_file.exists():
        return False

    try:
        lock_age = time.time() - lock_file.stat().st_mtime
    except OSError as e:
        _LOGGER.debug(f"Could not stat lock file {lock_file}: {e}")
        return False

    if lock_age < STALE_LOCK_MAX_AGE_SECONDS:
        _LOGGER.debug(
            f"Detected active/recent index.lock ({lock_age:.0f}s old): {lock_file}"
        )
        return False

    if os.getenv(STALE_LOCK_CLEANUP_ENV) != "1":
        _LOGGER.warning(
            f"Found stale-looking index.lock ({lock_age:.0f}s old): {lock_file}. "
            f"Skipping auto-removal. Set {STALE_LOCK_CLEANUP_ENV}=1 to enable cleanup."
        )
        return False

    _LOGGER.warning(f"Removing stale index.lock ({lock_age:.0f}s old): {lock_file}")
    try:
        lock_file.unlink()
    except OSError as e:
        _LOGGER.warning(f"Failed to remove lock file {lock_file}: {e}")
        return False
    return True


def warn_if_index_lock_present(worktree_path: Path) -> None:
    """Log a warning if the target worktree currently has an index.lock file."""
    git_dir = get_worktree_git_dir(worktree_path)
    if not git_dir:
        return
    lock_file = git_dir / "index.lock"
    if not lock_file.exists():
        return

    age = "unknown"
    with contextlib.suppress(OSError):
        age = f"{time.time() - lock_file.stat().st_mtime:.0f}s"
    _LOGGER.warning(f"index.lock present in target worktree ({age} old): {lock_file}")

    # Best-effort cleanup for stale locks if explicitly enabled.
    remove_stale_lock(worktree_path)


# =============================================================================
# Ninja Utilities
# =============================================================================


def get_ninja_commands(build_dir: Path) -> dict[str, str]:
    """Get expanded commands from ninja using 'ninja -t compdb'."""
    result = subprocess.run(
        ["ninja", "-t", "compdb"],
        cwd=build_dir,
        capture_output=True,
        text=True,
    )

    if result.returncode != 0:
        return {}

    output_to_command: dict[str, str] = {}
    try:
        entries = json.loads(result.stdout)
        for entry in entries:
            output = entry.get("output", "")
            command = entry.get("command", "")
            if output and command:
                # compdb gives relative paths, convert to match .ninja_log format
                if not output.startswith("/"):
                    output = str(build_dir / output)
                output_to_command[output] = command
                # Also store relative path version
                with contextlib.suppress(ValueError):
                    rel_output = str(Path(output).relative_to(build_dir))
                    output_to_command[rel_output] = command
    except json.JSONDecodeError:
        pass

    return output_to_command


def fix_ninja_log(
    ninja_log_path: Path, build_dir: Path, output_to_command: dict[str, str]
) -> None:
    """Recompute command hashes and mtimes in .ninja_log based on actual files."""
    if not ninja_log_path.exists():
        return

    build_dir_str = str(build_dir) + "/"
    lines = ninja_log_path.read_text().split("\n")
    new_lines = []

    for line in lines:
        if line.startswith("#") or not line.strip():
            new_lines.append(line)
            continue

        parts = line.split("\t")
        if len(parts) != 5:
            new_lines.append(line)
            continue

        start, end, old_mtime, output, old_hash = parts

        # Get the actual file mtime
        if output.startswith("/"):
            output_path = Path(output)
            lookup_key = output
        else:
            output_path = build_dir / output
            # Try both relative and absolute paths for command lookup
            lookup_key = build_dir_str + output

        if output_path.exists():
            new_mtime = output_path.stat().st_mtime_ns
        else:
            new_mtime = old_mtime

        # Look up command for this output and recompute hash
        # Try the original output path first, then the converted absolute path
        command = output_to_command.get(output) or output_to_command.get(lookup_key)
        if command:
            new_hash = rapidhash.rapidhash(command.encode())
            new_lines.append(f"{start}\t{end}\t{new_mtime}\t{output}\t{new_hash:x}")
        else:
            new_lines.append(f"{start}\t{end}\t{new_mtime}\t{output}\t{old_hash}")

    ninja_log_path.write_text("\n".join(new_lines))


def fix_ninja_deps(ninja_deps_path: Path, src_str: str, dst_str: str) -> None:
    """Fix paths in .ninja_deps binary file.

    Format: header line + 4-byte version + records
    Each record: 4-byte length + [string + null + padding + 4-byte ID]
    """
    if not ninja_deps_path.exists():
        return

    content = ninja_deps_path.read_bytes()
    src_bytes = src_str.encode()
    if src_bytes not in content:
        return

    dst_bytes = dst_str.encode()
    result = bytearray()

    # Copy header (first line + version)
    with contextlib.suppress(ValueError):
        header_end = content.index(b"\n") + 1 + 4
        if header_end <= len(content):
            result.extend(content[:header_end])
            i = header_end
        else:
            return
    if not result:
        return

    while i + 4 <= len(content):
        # Read record length (includes string + null + padding + 4-byte ID)
        rec_len = int.from_bytes(content[i : i + 4], "little")
        if rec_len < 4 or i + 4 + rec_len > len(content):
            result.extend(content[i:])
            break

        # Record data is rec_len bytes: string + null + padding + 4-byte ID
        rec_data = content[i + 4 : i + 4 + rec_len]
        str_part = rec_data[:-4]  # Everything except the 4-byte ID
        rec_id = rec_data[-4:]  # Last 4 bytes are the ID

        # Find null terminator in string part
        null_pos = str_part.find(b"\x00")
        if null_pos == -1:
            result.extend(content[i : i + 4 + rec_len])
            i += 4 + rec_len
            continue
        old_str = str_part[:null_pos]

        # Replace path if needed
        if src_bytes in old_str:
            new_str = old_str.replace(src_bytes, dst_bytes)
            new_str_padded_len = (
                (len(new_str) + 1) + 3
            ) & ~3  # +1 for null, align to 4
            new_rec_len = new_str_padded_len + 4  # +4 for ID
            result.extend(new_rec_len.to_bytes(4, "little"))
            result.extend(new_str)
            result.append(0)
            result.extend(b"\x00" * (new_str_padded_len - len(new_str) - 1))
            result.extend(rec_id)
        else:
            result.extend(content[i : i + 4 + rec_len])
        i += 4 + rec_len

    ninja_deps_path.write_bytes(bytes(result))


# =============================================================================
# Task System
# =============================================================================


class Task(ABC):
    """Base class for graft tasks."""

    name: str
    description: str

    @abstractmethod
    def should_run(self, source: Path, target: Path) -> bool:
        """Check if this task should run for the given worktrees."""
        ...

    @abstractmethod
    def run(self, source: Path, target: Path, spinner: Spinner) -> None:
        """Execute the task."""
        ...

    def get_subtasks(self) -> list[tuple[str, str]]:
        """Return list of (name, description) tuples for subtasks.

        Override this if the task has subtasks that should be shown in the spinner.
        """
        return [(self.name, self.description)]


class TimestampTask(Task):
    """Copy file timestamps from source to target worktree."""

    name = "timestamps"
    description = "copying file timestamps"

    def should_run(self, source: Path, target: Path) -> bool:
        # Always run if source exists
        return source.exists()

    def run(self, source: Path, target: Path, spinner: Spinner) -> None:
        def copy_timestamp(src_file: Path) -> None:
            rel_path = src_file.relative_to(source)
            dst_file = target / rel_path
            if dst_file.exists():
                try:
                    src_stat = src_file.stat()
                    os.utime(dst_file, (src_stat.st_atime, src_stat.st_mtime))
                except OSError:
                    pass  # Skip files that can't be stat'd or utime'd

        def is_regular_file(p: Path) -> bool:
            """Check if path is a regular file, handling stat errors gracefully."""
            try:
                return p.is_file() and not p.is_symlink()
            except OSError:
                return False

        files = [f for f in source.rglob("*") if is_regular_file(f)]
        with ThreadPoolExecutor(max_workers=8) as executor:
            list(executor.map(copy_timestamp, files))

        spinner.complete_task(self.name)


class SubmoduleTask(Task):
    """Copy submodules from source to target worktree."""

    name = "submodules"
    description = "copying submodules"

    def __init__(self) -> None:
        self._submodules_to_copy: list[tuple[str, Path, Path]] = []
        self._submodules_to_init: list[tuple[str, str]] = []
        self._git_modules_dir: Path | None = None

    def should_run(self, source: Path, target: Path) -> bool:
        # Check if target has .gitmodules
        gitmodules = target / ".gitmodules"
        return gitmodules.exists()

    def get_subtasks(self) -> list[tuple[str, str]]:
        return [
            (self.name, self.description),
            ("submodules_update", "updating submodules"),
        ]

    def _prepare(self, source: Path, target: Path) -> None:
        """Prepare submodule lists for copying."""
        self._submodules_to_copy = []
        self._submodules_to_init = []
        self._git_modules_dir = get_git_modules_dir(source)

        def has_entries(path: Path) -> bool:
            try:
                return path.is_dir() and any(path.iterdir())
            except OSError:
                return False

        for info in get_submodule_info(target):
            src = source / info.path
            dst = target / info.path
            src_populated = has_entries(src)
            dst_empty = not has_entries(dst)

            if src_populated and dst_empty:
                self._submodules_to_copy.append((info.path, src, dst))
            elif not src_populated and info.url:
                # Submodule not initialized in source - will need to clone
                _LOGGER.debug(
                    f"Will clone {info.path} (name={info.name}) from {info.url}"
                )
                self._submodules_to_init.append((info.path, info.url))

    def run(self, source: Path, target: Path, spinner: Spinner) -> None:
        self._prepare(source, target)
        self._copy_submodules(source)
        spinner.complete_task(self.name)

        spinner.start_task("submodules_update")
        self._update_submodule_commits(target)
        self._clone_missing_submodules(target)
        spinner.complete_task("submodules_update")

    def _copy_submodules(self, source: Path) -> None:
        """Copy submodule working trees and .git directories."""
        if not self._submodules_to_copy:
            return

        def ignore_git(directory: str, files: list[str]) -> list[str]:
            return [".git"] if ".git" in files else []

        def copy_submodule(args: tuple[str, Path, Path]) -> None:
            submodule_path, src, dst = args
            # Safety check: never modify source directory
            if dst.is_relative_to(source):
                raise RuntimeError(f"Refusing to copy into source tree: {dst}")
            if dst.exists():
                shutil.rmtree(dst)
            _LOGGER.debug(f"Copying submodule {submodule_path}")

            # Copy working tree files (exclude .git) with retry
            for attempt in range(3):
                try:
                    shutil.copytree(src, dst, symlinks=True, ignore=ignore_git)
                    break
                except (shutil.Error, OSError) as e:
                    if attempt == 2:
                        raise
                    _LOGGER.debug(f"Retrying copy of {submodule_path}: {e}")
                    if dst.exists():
                        shutil.rmtree(dst)
                    time.sleep(0.1)

            # Copy git data from shared modules to create standalone .git/ directory
            if self._git_modules_dir:
                src_git_dir = self._git_modules_dir / submodule_path
                dst_git_dir = dst / ".git"
                if src_git_dir.exists():
                    for attempt in range(3):
                        try:
                            shutil.copytree(src_git_dir, dst_git_dir, symlinks=True)
                            break
                        except (shutil.Error, OSError) as e:
                            if attempt == 2:
                                raise
                            _LOGGER.debug(
                                f"Retrying .git copy for {submodule_path}: {e}"
                            )
                            if dst_git_dir.exists():
                                shutil.rmtree(dst_git_dir)
                            time.sleep(0.1)

                    # Make all files writable (git pack files are read-only)
                    for f in dst_git_dir.rglob("*"):
                        if f.is_file():
                            with contextlib.suppress(OSError):
                                f.chmod(f.stat().st_mode | 0o200)

                    # Remove core.worktree - not needed when .git is inside working tree
                    config_file = dst_git_dir / "config"
                    unset_result = run_git(
                        ["config", "-f", str(config_file), "--unset", "core.worktree"],
                        cwd=dst,
                    )
                    if unset_result.returncode != 0:
                        _LOGGER.debug(
                            f"Could not unset core.worktree in {config_file}: "
                            f"{_stderr_text(unset_result.stderr).strip()}"
                        )

        with ThreadPoolExecutor(max_workers=4) as executor:
            list(executor.map(copy_submodule, self._submodules_to_copy))

    def _update_submodule_commits(self, target: Path) -> None:
        """Checkout the correct commit for each submodule.

        Uses ls-tree to get the commit the branch expects, not current state.
        Does not use `git submodule update` which modifies shared .git/modules config.
        """
        result = run_git(["ls-tree", "-rz", "HEAD"], cwd=target, text=False)
        if result.returncode != 0:
            _LOGGER.debug(
                f"Could not list submodule commits in {target}: "
                f"{_stderr_text(result.stderr).strip()}"
            )
            return

        for entry in result.stdout.split(b"\x00"):
            if not entry:
                continue
            # Format: <mode> <type> <object>\t<path>\0
            if b"\t" not in entry:
                continue
            meta, path_bytes = entry.split(b"\t", 1)
            parts = meta.decode(errors="replace").split()
            if len(parts) < 3 or parts[1] != "commit":
                continue
            commit = parts[2]
            path = path_bytes.decode(errors="surrogateescape")
            submodule_dir = target / path
            if not submodule_dir.exists():
                continue
            checkout_result = run_git(
                ["checkout", "--detach", "--quiet", commit],
                cwd=submodule_dir,
                retry_on_index_lock=True,
            )
            if checkout_result.returncode != 0:
                _LOGGER.warning(
                    f"Failed to checkout submodule {path} at {commit}: "
                    f"{_stderr_text(checkout_result.stderr).strip()}"
                )

    def _clone_missing_submodules(self, target: Path) -> None:
        """Clone submodules that weren't initialized in source."""
        for submodule_path, url in self._submodules_to_init:
            _LOGGER.debug(f"Cloning submodule {submodule_path} from {url}")
            submodule_dir = target / submodule_path

            # Get expected commit for this submodule
            commit_result = run_git(
                ["ls-tree", "HEAD", submodule_path],
                cwd=target,
            )
            if commit_result.returncode != 0:
                _LOGGER.debug(f"  ls-tree failed for {submodule_path}")
                continue
            parts = commit_result.stdout.split()
            if len(parts) < 3 or parts[1] != "commit":
                _LOGGER.debug(f"  not a commit entry: {commit_result.stdout}")
                continue
            commit = parts[2]
            _LOGGER.debug(f"  target commit: {commit}")

            # Clone and checkout
            if submodule_dir.exists():
                shutil.rmtree(submodule_dir)
            clone_result = run_git(
                ["clone", url, str(submodule_dir)],
                cwd=target,
            )
            if clone_result.returncode != 0:
                _LOGGER.debug(f"  clone failed: {clone_result.stderr}")
                continue

            # Fetch the specific commit if not on default branch
            fetch_result = run_git(
                ["fetch", "origin", commit],
                cwd=submodule_dir,
                retry_on_index_lock=True,
            )
            if fetch_result.returncode != 0:
                _LOGGER.debug(
                    f"  fetch failed for {submodule_path}@{commit}: "
                    f"{_stderr_text(fetch_result.stderr).strip()}"
                )

            checkout_result = run_git(
                ["checkout", "--detach", "--quiet", commit],
                cwd=submodule_dir,
                retry_on_index_lock=True,
            )
            if checkout_result.returncode != 0:
                _LOGGER.warning(
                    f"Failed to checkout cloned submodule {submodule_path} at {commit}: "
                    f"{_stderr_text(checkout_result.stderr).strip()}"
                )
                continue

            # Make all .git files writable (git pack files are read-only)
            git_dir = submodule_dir / ".git"
            if git_dir.is_dir():
                for f in git_dir.rglob("*"):
                    if f.is_file():
                        with contextlib.suppress(OSError):
                            f.chmod(f.stat().st_mode | 0o200)


class BuildTask(Task):
    """Copy build directories from source to target and fix paths."""

    name = "build"
    description = "processing build directory"

    def __init__(self) -> None:
        self._build_dirs_to_copy: list[tuple[Path, Path]] = []
        self._build_dirs_to_fix: list[Path] = []

    def should_run(self, source: Path, target: Path) -> bool:
        self._detect_build_dirs(source, target)
        return len(self._build_dirs_to_copy) > 0 or len(self._build_dirs_to_fix) > 0

    def get_subtasks(self) -> list[tuple[str, str]]:
        return [
            (self.name, self.description),
            ("cmake_paths", "fixing CMake paths"),
            ("ninja_files", "fixing ninja files"),
        ]

    def _detect_build_dirs(self, source: Path, target: Path) -> None:
        """Detect build directories to copy and existing directories to fix."""
        self._build_dirs_to_copy = []
        self._build_dirs_to_fix = []

        for pattern in BUILD_DIR_PATTERNS:
            if "*" in pattern:
                # Check source dirs to copy
                for src_dir in source.glob(pattern):
                    if src_dir.is_dir():
                        rel_path = src_dir.relative_to(source)
                        dst_dir = target / rel_path
                        if not dst_dir.exists():
                            self._build_dirs_to_copy.append((src_dir, dst_dir))
                # Check target dirs to fix (including target-only dirs)
                for dst_dir in target.glob(pattern):
                    if dst_dir.is_dir() and dst_dir not in self._build_dirs_to_fix:
                        self._build_dirs_to_fix.append(dst_dir)
            else:
                src_dir = source / pattern
                dst_dir = target / pattern
                if src_dir.is_dir() and not dst_dir.exists():
                    self._build_dirs_to_copy.append((src_dir, dst_dir))
                if dst_dir.is_dir() and dst_dir not in self._build_dirs_to_fix:
                    self._build_dirs_to_fix.append(dst_dir)

        # Remove parent directories if children are already in the lists
        # (e.g., don't copy/fix "build" if "build/release" is already listed)
        dst_paths = {dst for _, dst in self._build_dirs_to_copy}
        self._build_dirs_to_copy = [
            (src, dst)
            for src, dst in self._build_dirs_to_copy
            if not any(
                other != dst and other.is_relative_to(dst) for other in dst_paths
            )
        ]

        fix_paths = set(self._build_dirs_to_fix)
        self._build_dirs_to_fix = [
            dst
            for dst in self._build_dirs_to_fix
            if not any(
                other != dst and other.is_relative_to(dst) for other in fix_paths
            )
        ]

    def run(self, source: Path, target: Path, spinner: Spinner) -> None:
        src_str = str(source)
        dst_str = str(target)

        # Symlink CMakeUserPresets.json if it exists
        self._symlink_cmake_presets(source, target)

        # Copy all detected build directories
        copied_dirs = self._copy_build_dirs()
        spinner.complete_task(self.name)

        # Fix paths in both copied and existing directories
        all_dirs_to_fix = copied_dirs + self._build_dirs_to_fix

        if not all_dirs_to_fix:
            spinner.complete_task("cmake_paths")
            spinner.complete_task("ninja_files")
            return

        spinner.start_task("cmake_paths")
        self._fix_cmake_paths(all_dirs_to_fix, src_str, dst_str)
        spinner.complete_task("cmake_paths")

        spinner.start_task("ninja_files")
        self._fix_ninja_files(all_dirs_to_fix, src_str, dst_str)
        spinner.complete_task("ninja_files")

    def _symlink_cmake_presets(self, source: Path, target: Path) -> None:
        """Symlink CMakeUserPresets.json if it exists in source parent."""
        user_presets_src = source.parent / "CMakeUserPresets.json"
        user_presets_dst = target / "CMakeUserPresets.json"
        if user_presets_src.exists() and not user_presets_dst.exists():
            with contextlib.suppress(OSError):
                user_presets_dst.symlink_to(user_presets_src)

    def _copy_build_dirs(self) -> list[Path]:
        """Copy all detected build directories. Returns list of copied destination dirs."""
        copied = []
        for src_build, dst_build in self._build_dirs_to_copy:
            _LOGGER.debug(f"Copying {src_build.name}")
            for attempt in range(3):
                try:
                    shutil.copytree(src_build, dst_build, symlinks=True)
                    copied.append(dst_build)
                    break
                except (shutil.Error, OSError) as e:
                    if attempt == 2:
                        raise
                    _LOGGER.debug(f"Retrying build copy: {e}")
                    if dst_build.exists():
                        shutil.rmtree(dst_build)
                    time.sleep(0.1)
        return copied

    def _fix_cmake_paths(
        self, build_dirs: list[Path], src_str: str, dst_str: str
    ) -> None:
        """Fix paths in all CMake-generated files."""

        def fix_file(path: Path) -> None:
            try:
                content = path.read_text()
                if src_str in content:
                    stat = path.stat()
                    path.write_text(content.replace(src_str, dst_str))
                    os.utime(path, (stat.st_atime, stat.st_mtime))
            except (UnicodeDecodeError, OSError):
                pass  # Skip binary files

        for build_dir in build_dirs:
            files = [
                p for p in build_dir.rglob("*") if p.is_file() and not p.is_symlink()
            ]
            with ThreadPoolExecutor(max_workers=8) as executor:
                list(executor.map(fix_file, files))

    def _fix_ninja_files(
        self, build_dirs: list[Path], src_str: str, dst_str: str
    ) -> None:
        """Fix ninja log hashes and deps paths in all build directories."""
        for build_dir in build_dirs:
            # Run ninja_deps fixing and get_ninja_commands in parallel
            with ThreadPoolExecutor(max_workers=2) as executor:
                deps_future = executor.submit(
                    fix_ninja_deps, build_dir / ".ninja_deps", src_str, dst_str
                )
                commands_future = executor.submit(get_ninja_commands, build_dir)
                deps_future.result()
                output_to_command = commands_future.result()

            # Recompute command hashes in .ninja_log
            ninja_log = build_dir / ".ninja_log"
            if ninja_log.exists():
                fix_ninja_log(ninja_log, build_dir, output_to_command)


class ClaudeSettingsTask(Task):
    """Copy Claude Code settings and set CLAUDE_CODE_TASK_LIST_ID."""

    name = "claude_settings"
    description = "configuring Claude settings"

    def __init__(
        self, remote_url: str | None = None, branch: str | None = None
    ) -> None:
        self._remote_url = remote_url
        self._branch = branch

    def should_run(self, source: Path, target: Path) -> bool:
        settings_src = source / ".claude" / "settings.local.json"
        # Run if source settings exist OR if we can set task list ID
        return settings_src.exists() or (self._remote_url and self._branch)

    def run(self, source: Path, target: Path, spinner: Spinner) -> None:
        settings_src = source / ".claude" / "settings.local.json"
        settings_dst = target / ".claude" / "settings.local.json"

        # Create .claude directory if needed
        settings_dst.parent.mkdir(parents=True, exist_ok=True)

        # Start with existing settings or empty dict
        settings: dict = {}
        if settings_src.exists():
            shutil.copy2(settings_src, settings_dst)
            try:
                settings = json.loads(settings_dst.read_text())
            except (json.JSONDecodeError, OSError):
                settings = {}
        elif settings_dst.exists():
            try:
                settings = json.loads(settings_dst.read_text())
            except (json.JSONDecodeError, OSError):
                settings = {}

        # Set task list ID if we have remote URL and branch
        task_list_id = self._build_task_list_id()
        if task_list_id:
            if "env" not in settings:
                settings["env"] = {}
            settings["env"]["CLAUDE_CODE_TASK_LIST_ID"] = task_list_id
            settings_dst.write_text(json.dumps(settings, indent=2) + "\n")
            _LOGGER.debug(f"Set CLAUDE_CODE_TASK_LIST_ID={task_list_id}")

        spinner.complete_task(self.name)

    def _build_task_list_id(self) -> str | None:
        """Build task list ID from remote URL and branch."""
        if not self._remote_url or not self._branch:
            return None

        parsed = parse_org_repo_from_url(self._remote_url)
        if not parsed:
            _LOGGER.debug(f"Could not parse org/repo from URL: {self._remote_url}")
            return None

        org, repo = parsed
        sanitized_branch = sanitize_for_id(self._branch)
        return f"{org}-{repo}-{sanitized_branch}"


# =============================================================================
# Task Runner
# =============================================================================


@dataclass
class TaskRunner:
    """Orchestrates parallel execution of graft tasks."""

    source: Path
    target: Path
    remote_url: str | None = None
    branch: str | None = None
    tasks: list[Task] = field(default_factory=list)

    def __post_init__(self) -> None:
        # Register all task types
        self.tasks = [
            TimestampTask(),
            SubmoduleTask(),
            BuildTask(),
            ClaudeSettingsTask(remote_url=self.remote_url, branch=self.branch),
        ]

    def get_applicable_tasks(self) -> list[Task]:
        """Return list of tasks that should run for this worktree pair."""
        return [t for t in self.tasks if t.should_run(self.source, self.target)]

    def run(self) -> None:
        """Run all applicable tasks in parallel."""
        # Report lock state up front and optionally clean stale lock files.
        warn_if_index_lock_present(self.target)

        applicable = self.get_applicable_tasks()
        if not applicable:
            _LOGGER.debug("No tasks to run")
            return

        # Collect all subtasks for spinner display
        all_subtasks: list[tuple[str, str]] = []
        initial_active: list[str] = []
        for task in applicable:
            subtasks = task.get_subtasks()
            all_subtasks.extend(subtasks)
            # Mark first subtask of each task as initially active
            if subtasks:
                initial_active.append(subtasks[0][0])

        s = Spinner()
        s.set_tasks(all_subtasks, active=initial_active)
        s.start()

        try:
            # Run all tasks in parallel
            # Tasks operate on disjoint paths:
            # - TimestampTask: existing tracked files only
            # - SubmoduleTask: <submodule>/ directories
            # - BuildTask: build/*/ directories
            # - ClaudeSettingsTask: .claude/ directory
            with ThreadPoolExecutor(max_workers=len(applicable)) as executor:
                futures = {
                    executor.submit(task.run, self.source, self.target, s): task
                    for task in applicable
                }
                error = None
                for future in futures:
                    try:
                        future.result()
                    except Exception as e:
                        if error is None:
                            error = (futures[future], e)
                if error:
                    s.stop()
                    task, exc = error
                    print(f"{RED}{CROSS}{RESET} {task.description}")
                    print(f"  {exc}")
                    sys.exit(1)
        finally:
            s.stop()


# =============================================================================
# CLI
# =============================================================================


def detect_remote_url(worktree_path: Path) -> str | None:
    """Detect the remote URL from git origin."""
    result = run_git(["remote", "get-url", "origin"], cwd=worktree_path)
    if result.returncode == 0:
        return result.stdout.strip()
    return None


def detect_branch(worktree_path: Path) -> str | None:
    """Detect the current branch name."""
    result = run_git(["branch", "--show-current"], cwd=worktree_path)
    if result.returncode == 0 and result.stdout.strip():
        return result.stdout.strip()
    return None


@click.command()
@click.argument("worktree_path", type=click.Path(exists=True, resolve_path=True))
@click.option(
    "-s",
    "--source",
    type=click.Path(exists=True, resolve_path=True),
    help="Source worktree path (default: auto-detect primary worktree)",
)
@click.option(
    "--remote-url",
    help="Git remote URL (default: auto-detect from origin)",
)
@click.option(
    "--branch",
    help="Branch name (default: auto-detect current branch)",
)
@click.option("-v", "--verbose", is_flag=True, help="Enable verbose output")
def main(
    worktree_path: str,
    source: str | None,
    remote_url: str | None,
    branch: str | None,
    verbose: bool,
) -> None:
    """Optimize a new git worktree by grafting cached state from the primary worktree."""
    _LOGGER.setLevel(logging.DEBUG if verbose else logging.INFO)

    target = Path(worktree_path)

    # Find source worktree
    if source:
        source_path = Path(source)
    else:
        source_path = find_primary_worktree(target)

    if source_path is None:
        _LOGGER.debug("No source worktree found, nothing to graft")
        sys.exit(0)

    # Validate worktrees
    validation_error = validate_worktrees(source_path, target)
    if validation_error:
        print(f"{RED}{CROSS}{RESET} {validation_error}")
        sys.exit(1)

    # Auto-detect remote URL and branch if not provided
    if not remote_url:
        remote_url = detect_remote_url(target)
        if not remote_url:
            _LOGGER.debug("Could not detect remote URL, skipping task list ID setup")
    if not branch:
        branch = detect_branch(target)
        if not branch:
            _LOGGER.debug(
                "Could not detect branch (detached HEAD?), skipping task list ID setup"
            )

    _LOGGER.debug(f"Source: {source_path}")
    _LOGGER.debug(f"Target: {target}")
    if remote_url:
        _LOGGER.debug(f"Remote URL: {remote_url}")
    if branch:
        _LOGGER.debug(f"Branch: {branch}")

    # Run all applicable tasks
    runner = TaskRunner(
        source=source_path,
        target=target,
        remote_url=remote_url,
        branch=branch,
    )
    runner.run()


if __name__ == "__main__":
    main()
